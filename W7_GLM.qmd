---
title: "The General Linear Model"
subtitle: "Biostatistics"
author: "Dr. Lea Hildebrandt"
format: 
  revealjs:
    smaller: true
    scrollable: true
    slide-number: true
    theme: serif
    chalkboard: true
editor: visual
from: markdown+emoji
---

# The General Linear Model!

```{r}
#| message: false

library(tidyverse)
library(ggplot2)
library(fivethirtyeight)
#library(caret)
library(MASS)
library(cowplot)
library(knitr)
set.seed(123456) # set random seed to exactly replicate results
opts_chunk$set(tidy.opts=list(width.cutoff=80))
options(tibble.width = 60)
# load the NHANES data library
library(NHANES)
# drop duplicated IDs within the NHANES dataset
NHANES <- 
  NHANES %>% 
  dplyr::distinct(ID,.keep_all=TRUE)
NHANES_adult <- 
  NHANES %>%
  drop_na(Weight) %>%
  subset(Age>=18)
```

Remember the basic model of statistics:

$$
data = model + error
$$

Our general goal is to find the model with the *best fit*, i.e. that minimizes the error.

One approach is the GLM. You might be surprised that a lot of the common models can be viewed as linear models:

::: scrollable
![All models can be thought of as linear models](images/linear_tests_cheat_sheet.png){alt="All models can be thought of as linear models"}
:::

::: notes
finally!
:::

## Definitions

**Dependent variable (DV)**: The outcome variable that the model aims to explain ($Y$).

**Independent variable (IV)**: The variable that we use to explain the DV ($X$).

**Linear model**: The model for the DV is composed of a *linear combination* of IVs (that are multiplied by different [weights]{.underline}!)

. . .

The weights are the *parameters* $\beta$ and determine the relative contribution of each IV. (This is what the model estimates! The weights thus give us the important information we're usually interested in: How strong are IV and DV related.)

There may be several DVs, but usually that's not the case and we will focus on those cases with one DV!

## Example

::: columns
::: column
Let's use some simulated data:

```{r}
#| echo: false
#| fig.width: 3
#| fig.height: 3
#| out.height: '50%'
#| 
# create simulated data for example
set.seed(12345)
# the number of points that having a prior class increases grades
betas <- c(6, 5)
df <-
  tibble(
    studyTime = c(2, 3, 5, 6, 6, 8, 10, 12) / 3,
    priorClass = c(0, 1, 1, 0, 1, 0, 1, 0)
  ) %>%
  mutate(
    grade = 
      studyTime * betas[1] + 
      priorClass * betas[2] + 
      round(rnorm(8, mean = 70, sd = 5))
  )

p <- ggplot(df,aes(studyTime,grade)) +
  geom_point(size=3) +
  xlab('Study time (hours)') +
  ylab('Grade (percent)') +
  xlim(0,5) + 
  ylim(70,100)
print(p)
```
:::

::: column
We can calculate the *correlation* between the two variables:

```{r}
#| echo: false
# compute correlation between grades and study time
corTestResult <- cor.test(df$grade, df$studyTime)
corTestResult
```

The correlation is quite high (.63), but the CI is also pretty wide.
:::
:::

. . .

Fundamental activities of statistics:

-   *Describe*: How strong is the relationship between grade and study time?

-   *Decide*: Is there a statistically significant relationship between grade and study time?

-   *Predict*: Given a particular amount of study time, what grade do we expect?

::: notes
relationship study time and grade
:::

## Linear Regression

Use the GLM (\~synonymous to linear regression) to...

::: incremental
-   decribe the relation between two variables (similar to correlation)

-   predict DV for new values of IV (new observations)

-   add multiple IVs!
:::

. . .

::: columns
::: column
Simple GLM:

$$
y = \beta_0+ x * \beta_x + \epsilon
$$

$\beta_0$ = *intercept*, the overall offset of the line when $x=0$ (even if that is impossible)\
$\beta_x$ = *slope*, how much do we expect $y$ to change with each change in $x$?\
$y$ = *DV*\
$x$ = *IV* or *predictor\
*$\epsilon$ = *error term*, whatever variance is left over once the model is fit, *residuals*! (Think of the model as the line that is fitted and the residuals are the vertical deviations of the data points from the line!)

(If we refer to *predicted* $y$-values, after we have estimated the model fit/line, we can drop the error term: $\hat{y} = \hat{\beta_0} + x * \hat{\beta_x}$.)
:::

::: column
```{r}
#| echo: false

lmResult <- lm(grade~studyTime,data=df)
p2 <- p+geom_abline(slope=lmResult$coefficients[2],
                  intercept=lmResult$coefficients[1],
                  color='blue')
p3 <- p2 +
  geom_hline(yintercept=lmResult$coefficients[1],color='black',size=0.5,linetype='dotted') +
  annotate('segment',x=2,xend=3,color='red',linetype='dashed',
           y=predict(lmResult,newdata=data.frame(studyTime=2))[1],
           yend=predict(lmResult,newdata=data.frame(studyTime=2))[1]) +
   annotate('segment',x=3,xend=3,color='red',linetype='dashed',
           y=predict(lmResult,newdata=data.frame(studyTime=2))[1],
           yend=predict(lmResult,newdata=data.frame(studyTime=3))[1])
 
print(p3)

```
:::
:::

## The Relation Between Correlation and Regression

There is a close relation and we can convert $r$ to $\hat{\beta_x}$.

$\hat{r} = \frac{covariance_{xy}}{s_x * s_y}$

$\hat{\beta_x} = \frac{covariance_{xy}}{s_x*s_x}$

$covariance_{xy} = \hat{r} * s_x * s_y$

$\hat{\beta_x} = \frac{\hat{r} * s_x * s_y}{s_x * s_x} = r * \frac{s_y}{s_x}$

\--\> Regression slope = correlation multiplied by ratio of SDs (if SDs are equal, $r$ = $\hat{\beta}$ )

::: notes
Estimation of GLM:

linear algebra (R will do that for us!) --\> Appendix book
:::

## Standard Errors for Regression Models

We usually want to make inferences about the regression parameter estimates. For this we need an estimate of their variability.

We first need an estimate of how much variability is *not* explained by the model: the **residual variance** (or **error variance**):

Compute *residuals*:

$$
residual = y - \hat{y} = y - (x*\hat{\beta_x} + \hat{\beta_0})
$$

Compute *Sum of Squared Errors* (remember?):

$$
SS_{error} = \sum_{i=1}^n{(y_i - \hat{y_i})^2} = \sum_{i=1}^n{residuals^2}
$$

Compute *Mean Squared Error*:

$$
MS_{error} = \frac{SS_{error}}{df} = \frac{\sum_{i=1}^n{(y_i - \hat{y_i})^2} }{N - p}
$$

where the $df$ are the number of observations $N$ - the number of estimated parameter $p$ (in this case 2: $\hat{\beta_0}$ and $\hat{\beta_x}$).

Finally, we can calculate the *standard error* for the *full* model:

$$
SE_{model} = \sqrt{MS_{error}}
$$

We can also calculate the SE for specific regression parameter estimates by rescaling the $SE_{model}$:

$$
SE_{\hat{\beta_x}} = \frac{SE_{model}}{\sqrt{\epsilon{(x_i - \bar{x})^2}}}
$$

::: notes
rescaling SE: by square root of the SS of the X variable
:::

## Statistical Tests for Regression Parameters

With the parameter estimates and their standard errors, we can compute $t$-statistics, which represent the likelihood of the observed estimate vs. the expected value under $H_0$ (usually 0, no effect).

$$
\begin{array}{c}
t_{N - p} = \frac{\hat{\beta} - \beta_{expected}}{SE_{\hat{\beta}}}\\
t_{N - p} = \frac{\hat{\beta} - 0}{SE_{\hat{\beta}}}\\
t_{N - p} = \frac{\hat{\beta} }{SE_{\hat{\beta}}}
\end{array}
$$

Usually, we would just let R do the calculations:

```{r}
summary(lmResult)
```

The intercept is significantly different from zero (which is usually not very relevant) and the effect of `studyTime` is not significant. So for every hour that we study more, the effect on the grade is rather small (4%) but possibly not present.

::: notes
$t$ ratio of $\beta$ to its $SE$!

intercept: expected grade without studying at all
:::

## Quantifying Goodness of Fit of the Model

Often, it is useful to check how good the model we estimated fits the data.

. . .

We can do that easily by asking *how much of the variability in the data is accounted for by the model?*

. . .

If we only have one IV ($x$), then we can simply square the correlation coefficient:

$$
R^2 = r^2
$$

In study time example, $R^2$ = 0.4 --\> we accounted for 40% of the overall variance in grades!

. . .

More generally, we can calculate $R^2$ with the Sum of Squared Variances:

$$
R^2 = \frac{SS_{model}}{SS_{total}} = 1-\frac{SS_{error}}{SS_{total}}
$$

::: notes
$R^2$ is the name of the GoF stat!

A small RÂ² tells us that even though a model might be significant, it may only explain a small amount of information in the DV
:::

## Fitting More Complex Models

Often we want to know the effects of *multiple variables* (IVs) on some outcome.

Example:\
Some students have taken a very similar class before, so there might not only be the effect of `studyTime` on `grades`, but also of having taken a `priorClass`.

. . .

::: columns
::: column
We can built a model that takes both into account by simply adding the "weight" and the IV (`priorClass`) to the model:

$\hat{y} = \hat{\beta_1}studyTime + \hat{\beta_2}priorClass + \hat{\beta_0}$
:::

::: column
If we plot the data, we can see that both IVs seem to have an effect on grades:

```{r, fig.width=5, fig.height=3}
df$priorClass <- as.factor(df$priorClass)
lmResultTwoVars <- lm(grade ~ studyTime + priorClass, data = df)
# summary(lmResultTwoVars)

p <- ggplot(df,aes(studyTime,grade,shape=priorClass)) +
  geom_point(size=3) + xlim(0,5) + ylim(70,100)
p <- p+
  geom_abline(slope=lmResultTwoVars$coefficients[2],
              intercept=lmResultTwoVars$coefficients[1],lineype='dotted')
# p <- p+
#   annotate('segment',x=2,xend=3,
#            y=lmResultTwoVars$coefficients[1]+
#              2*lmResultTwoVars$coefficients[2],
#            yend=lmResultTwoVars$coefficients[1]+
#              2*lmResultTwoVars$coefficients[2],
#            color='blue') +
#   annotate('segment',x=3,xend=3,
#            y=lmResultTwoVars$coefficients[1]+
#              2*lmResultTwoVars$coefficients[2],
#            yend=lmResultTwoVars$coefficients[1]+
#              3*lmResultTwoVars$coefficients[2],
#            color='blue')
p <- p+
  geom_abline(slope=lmResultTwoVars$coefficients[2],
              intercept=lmResultTwoVars$coefficients[1]+
                lmResultTwoVars$coefficients[3],
              linetype='dashed') 
p <- p+
  annotate('segment',x=2,xend=2,
           y=lmResultTwoVars$coefficients[1]+
             2*lmResultTwoVars$coefficients[2],
           yend=lmResultTwoVars$coefficients[1]+
             lmResultTwoVars$coefficients[3] +
             2*lmResultTwoVars$coefficients[2],
           linetype='dotted',size=1) +
  scale_color_discrete(
    limits = c(0, 1),
    labels = c("No", "Yes")
  ) +
  labs(
    color = "Previous course"
  )
print(p)
```
:::
:::
